{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the below command to download all the datasets and the relevant resources in one go\n",
    "# !python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "# negative_tweets.json: 5000 tweets with negative sentiments\n",
    "# positive_tweets.json: 5000 tweets with positive sentiments\n",
    "# tweets.20150430-223406.json: 20000 tweets with no sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday',\n",
       " '@France_Inte',\n",
       " '@PKuchly57',\n",
       " '@Milipol_Paris',\n",
       " 'for',\n",
       " 'being',\n",
       " 'top',\n",
       " 'engaged',\n",
       " 'members',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweet = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweet = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "\n",
    "pos_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "pos_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data\n",
    "\n",
    "1. Stemming is a process of removing affixes from a word. Stemming, working with only simple verb forms, is a heuristic process that removes the ends of words.\n",
    "    \n",
    "2. Lemmatization normalizes a word with the context of vocabulary and morphological analysis of words in text.   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#FollowFriday', 'JJ'),\n",
       " ('@France_Inte', 'NNP'),\n",
       " ('@PKuchly57', 'NNP'),\n",
       " ('@Milipol_Paris', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('top', 'JJ'),\n",
       " ('engaged', 'VBN'),\n",
       " ('members', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('community', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('week', 'NN'),\n",
       " (':)', 'NN')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "pos_tag(pos_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday',\n",
       " '@France_Inte',\n",
       " '@PKuchly57',\n",
       " '@Milipol_Paris',\n",
       " 'for',\n",
       " 'be',\n",
       " 'top',\n",
       " 'engage',\n",
       " 'member',\n",
       " 'in',\n",
       " 'my',\n",
       " 'community',\n",
       " 'this',\n",
       " 'week',\n",
       " ':)']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalized_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    normalized_tokens = []\n",
    "#     print(tokens)\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        normalized_tokens.append(lemmatizer.lemmatize(word, pos))\n",
    "    return normalized_tokens\n",
    "\n",
    "normalized_tokens(pos_tokens[0])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Noise from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words):\n",
    "    cleaned_tokens = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "   \n",
    "remove_noise(pos_tokens[0], eng_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cleaned_tokens = []\n",
    "neg_cleaned_tokens = []\n",
    "\n",
    "for tokens in pos_tokens:\n",
    "    pos_cleaned_tokens.append(remove_noise(tokens, eng_stop_words))\n",
    "\n",
    "for tokens in neg_tokens:\n",
    "    neg_cleaned_tokens.append(remove_noise(tokens, eng_stop_words))\n",
    "\n",
    "pos_cleaned_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_all_words at 0x000001D73634DCC8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or you could tokenize it and send it together to FreqDist method\n",
    "def get_all_words(cleaned_tweets):\n",
    "    for tweet in cleaned_tweets:\n",
    "        for token in tweet:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(pos_cleaned_tokens)\n",
    "all_pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':)', 3691),\n",
       " (':-)', 701),\n",
       " (':d', 658),\n",
       " ('thanks', 388),\n",
       " ('follow', 357),\n",
       " ('love', 333),\n",
       " ('...', 290),\n",
       " ('good', 283),\n",
       " ('get', 263),\n",
       " ('thank', 253),\n",
       " ('u', 245),\n",
       " ('day', 242),\n",
       " ('like', 229),\n",
       " ('see', 195),\n",
       " ('happy', 192),\n",
       " (\"i'm\", 183),\n",
       " ('great', 175),\n",
       " ('hi', 173),\n",
       " ('go', 167),\n",
       " ('back', 163)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "pos_freq = FreqDist(all_pos_words)\n",
    "pos_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_with_labels(cleaned_tweets):\n",
    "    for tweet in cleaned_tweets:\n",
    "        yield dict([token, True] for token in tweet)\n",
    "            \n",
    "pos_cleaned_labels = get_tweets_with_labels(pos_cleaned_tokens)\n",
    "neg_cleaned_labels = get_tweets_with_labels(neg_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sure': True, ':)': True}, 'Positive')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in pos_cleaned_labels]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in neg_cleaned_labels]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9953333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2081.7 : 1.0\n",
      "                follower = True           Positi : Negati =     32.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     32.9 : 1.0\n",
      "                     bam = True           Positi : Negati =     19.9 : 1.0\n",
      "                  arrive = True           Positi : Negati =     18.6 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.0 : 1.0\n",
      "               community = True           Positi : Negati =     15.3 : 1.0\n",
      "                 welcome = True           Positi : Negati =     15.2 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.0 : 1.0\n",
      "                    blog = True           Positi : Negati =     12.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
